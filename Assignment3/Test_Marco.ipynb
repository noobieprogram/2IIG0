{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 37\n",
      "True negatives: 39\n",
      "False positives: 2\n",
      "False negatives: 4\n",
      "Count correct: 76\n",
      "Num epochs: 25\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "class NeuralNetwork:\n",
    "# loss: cross/mse/msa/kl/huber (https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#id14)\n",
    "# activation: sigmoid/relu/leaky/elu/hyperbolic (lecture 11, slide 39)\n",
    "    def __init__(self, num_inputs, num_hidden_1, num_hidden_2, num_outputs, loss_function = \"cross\", learning_rate=0.5, hidden_layer_1_weights = None, hidden_layer_1_activation = \"elu\", hidden_layer_1_bias = None, hidden_layer_2_weights = None, hidden_layer_2_activation = \"sigmoid\", hidden_layer_2_bias = None, output_layer_weights = None, output_layer_activation = \"sigmoid\", output_layer_bias = None):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.loss_function = loss_function\n",
    "        self.hidden_layer_1 = NeuronLayer(num_hidden_1, loss_function, hidden_layer_1_activation, hidden_layer_1_bias)\n",
    "        self.hidden_layer_2 = NeuronLayer(num_hidden_2, loss_function, hidden_layer_2_activation, hidden_layer_2_bias)\n",
    "        self.output_layer = NeuronLayer(num_outputs, loss_function, output_layer_activation, output_layer_bias)\n",
    "        self.init_weights_from_inputs_to_hidden_layer_1_neurons(hidden_layer_1_weights)\n",
    "        self.init_weights_from_hidden_layer_1_to_hidden_layer_2_neurons(hidden_layer_2_weights)\n",
    "        self.init_weights_from_hidden_layer_2_neurons_to_output_layer_neurons(output_layer_weights)\n",
    "        self.init_bias_from_inputs_to_hidden_layer_1_neurons(hidden_layer_1_bias)\n",
    "        self.init_bias_from_hidden_layer_1_to_hidden_layer_2_neurons(hidden_layer_2_bias)\n",
    "        self.init_bias_from_hidden_layer_2_neurons_to_output_layer_neurons(output_layer_bias)\n",
    "        self.LEARNING_RATE = learning_rate\n",
    "\n",
    "    def init_weights_from_inputs_to_hidden_layer_1_neurons(self, hidden_layer_1_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer_1.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if not hidden_layer_1_weights:\n",
    "                    self.hidden_layer_1.neurons[h].weights.append(random.random()*2-1)\n",
    "                else:\n",
    "                    self.hidden_layer_1.neurons[h].weights.append(hidden_layer_1_weights[weight_num])\n",
    "                weight_num += 1\n",
    "                self.hidden_layer_1.neurons[h].gradient.append(0)\n",
    "                \n",
    "    def init_weights_from_hidden_layer_1_to_hidden_layer_2_neurons(self, hidden_layer_2_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer_2.neurons)):\n",
    "            for i in range(len(self.hidden_layer_1.neurons)):\n",
    "                if not hidden_layer_2_weights:\n",
    "                    self.hidden_layer_2.neurons[h].weights.append(random.random()*2-1)\n",
    "                else:\n",
    "                    self.hidden_layer_2.neurons[h].weights.append(hidden_layer_2_weights[weight_num])\n",
    "                weight_num += 1\n",
    "                self.hidden_layer_2.neurons[h].gradient.append(0)\n",
    "\n",
    "    def init_weights_from_hidden_layer_2_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for h in range(len(self.hidden_layer_2.neurons)):\n",
    "                if not output_layer_weights:\n",
    "                    self.output_layer.neurons[o].weights.append(random.random()*2-1)\n",
    "                else:\n",
    "                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "                self.output_layer.neurons[o].gradient.append(0)\n",
    "                \n",
    "    def init_bias_from_inputs_to_hidden_layer_1_neurons(self, hidden_layer_1_bias):\n",
    "        bias_num = 0\n",
    "        for h in range(len(self.hidden_layer_1.neurons)):\n",
    "            if not hidden_layer_1_bias:\n",
    "                self.hidden_layer_1.neurons[h].bias = random.random()*2-1\n",
    "            else:\n",
    "                self.hidden_layer_1.neurons[h].bias = hidden_layer_1_bias[bias_num]\n",
    "            bias_num += 1\n",
    "                \n",
    "    def init_bias_from_hidden_layer_1_to_hidden_layer_2_neurons(self, hidden_layer_2_bias):\n",
    "        bias_num = 0\n",
    "        for h in range(len(self.hidden_layer_2.neurons)):\n",
    "            if not hidden_layer_2_bias:\n",
    "                self.hidden_layer_2.neurons[h].bias = random.random()*2-1\n",
    "            else:\n",
    "                self.hidden_layer_2.neurons[h].bias = hidden_layer_2_bias[bias_num]\n",
    "            bias_num += 1\n",
    "\n",
    "    def init_bias_from_hidden_layer_2_neurons_to_output_layer_neurons(self, output_layer_bias):\n",
    "        bias_num = 0\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            if not output_layer_bias:\n",
    "                self.output_layer.neurons[o].bias = random.random()*2-1\n",
    "            else:\n",
    "                self.output_layer.neurons[o].bias = output_layer_bias[bias_num]\n",
    "            bias_num += 1\n",
    "\n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer 1')\n",
    "        self.hidden_layer_1.inspect()\n",
    "        print('------')\n",
    "        print('Hidden Layer 2')\n",
    "        self.hidden_layer_2.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "        print('------')\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_1_outputs = self.hidden_layer_1.feed_forward(inputs)\n",
    "        hidden_layer_2_outputs = self.hidden_layer_2.feed_forward(hidden_layer_1_outputs)\n",
    "        return self.output_layer.feed_forward(hidden_layer_2_outputs)\n",
    "\n",
    "    def train(self, training_sets):\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n",
    "            for o in range(len(self.output_layer.neurons)):\n",
    "                pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n",
    "                # if self.loss_function == \"mse\" or self.loss_function == \"msa\":\n",
    "                pd_errors_wrt_output_neuron_total_net_input[o]/=len(training_sets)\n",
    "            \n",
    "            pd_errors_wrt_hidden_2_neuron_total_net_input = [0] * len(self.hidden_layer_2.neurons)\n",
    "            for h in range(len(self.hidden_layer_2.neurons)):\n",
    "                d_error_wrt_hidden_2_neuron_output = 0\n",
    "                for o in range(len(self.output_layer.neurons)):\n",
    "                    d_error_wrt_hidden_2_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n",
    "                pd_errors_wrt_hidden_2_neuron_total_net_input[h] = d_error_wrt_hidden_2_neuron_output * self.hidden_layer_2.neurons[h].calculate_pd_total_net_input_wrt_input()\n",
    "            \n",
    "            pd_errors_wrt_hidden_1_neuron_total_net_input = [0] * len(self.hidden_layer_1.neurons)\n",
    "            for h in range(len(self.hidden_layer_1.neurons)):\n",
    "                d_error_wrt_hidden_1_neuron_output = 0\n",
    "                for o in range(len(self.hidden_layer_2.neurons)):\n",
    "                    d_error_wrt_hidden_1_neuron_output += pd_errors_wrt_hidden_2_neuron_total_net_input[o] * self.hidden_layer_2.neurons[o].weights[h]\n",
    "                pd_errors_wrt_hidden_1_neuron_total_net_input[h] = d_error_wrt_hidden_1_neuron_output * self.hidden_layer_1.neurons[h].calculate_pd_total_net_input_wrt_input()\n",
    "            \n",
    "            for o in range(len(self.output_layer.neurons)):\n",
    "                for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
    "                    if t == 0:\n",
    "                        self.output_layer.neurons[o].gradient[w_ho] = 0\n",
    "                    pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n",
    "                    self.output_layer.neurons[o].gradient[w_ho] += self.LEARNING_RATE * pd_error_wrt_weight\n",
    "                if t == 0:\n",
    "                    self.output_layer.neurons[o].bias_gradient = 0\n",
    "                pd_error_wrt_bias = pd_errors_wrt_output_neuron_total_net_input[o]\n",
    "                self.output_layer.neurons[o].bias_gradient += self.LEARNING_RATE * pd_error_wrt_bias\n",
    "            \n",
    "            for h in range(len(self.hidden_layer_2.neurons)):\n",
    "                for w_hh in range(len(self.hidden_layer_2.neurons[h].weights)):\n",
    "                    if t == 0:\n",
    "                        self.hidden_layer_2.neurons[h].gradient[w_hh] = 0\n",
    "                    pd_error_wrt_weight = pd_errors_wrt_hidden_2_neuron_total_net_input[h] * self.hidden_layer_2.neurons[h].calculate_pd_total_net_input_wrt_weight(w_hh)\n",
    "                    self.hidden_layer_2.neurons[h].gradient[w_hh] += self.LEARNING_RATE * pd_error_wrt_weight\n",
    "                if t == 0:\n",
    "                    self.hidden_layer_2.neurons[h].bias_gradient = 0\n",
    "                pd_error_wrt_bias = pd_errors_wrt_hidden_2_neuron_total_net_input[h]\n",
    "                self.hidden_layer_2.neurons[h].bias_gradient += self.LEARNING_RATE * pd_error_wrt_bias\n",
    "            \n",
    "            for h in range(len(self.hidden_layer_1.neurons)):\n",
    "                for w_ih in range(len(self.hidden_layer_1.neurons[h].weights)):\n",
    "                    if t == 0:\n",
    "                        self.hidden_layer_1.neurons[h].gradient[w_ih] = 0\n",
    "                    pd_error_wrt_weight = pd_errors_wrt_hidden_1_neuron_total_net_input[h] * self.hidden_layer_1.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n",
    "                    self.hidden_layer_1.neurons[h].gradient[w_ih] += self.LEARNING_RATE * pd_error_wrt_weight\n",
    "                if t == 0:\n",
    "                    self.hidden_layer_1.neurons[h].bias_gradient = 0\n",
    "                pd_error_wrt_bias = pd_errors_wrt_hidden_1_neuron_total_net_input[h]\n",
    "                self.hidden_layer_1.neurons[h].bias_gradient += self.LEARNING_RATE * pd_error_wrt_bias\n",
    "                    \n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            self.output_layer.neurons[o].bias -= self.output_layer.neurons[o].bias_gradient\n",
    "            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
    "                self.output_layer.neurons[o].weights[w_ho] -= self.output_layer.neurons[o].gradient[w_ho]\n",
    "        \n",
    "        for h in range(len(self.hidden_layer_2.neurons)):\n",
    "            self.hidden_layer_2.neurons[h].bias -= self.hidden_layer_2.neurons[h].bias_gradient\n",
    "            for w_hh in range(len(self.hidden_layer_2.neurons[h].weights)):\n",
    "                self.hidden_layer_2.neurons[h].weights[w_hh] -= self.hidden_layer_2.neurons[h].gradient[w_hh]\n",
    "            \n",
    "        for h in range(len(self.hidden_layer_1.neurons)):\n",
    "            self.hidden_layer_1.neurons[h].bias -= self.hidden_layer_1.neurons[h].bias_gradient\n",
    "            for w_ih in range(len(self.hidden_layer_1.neurons[h].weights)):\n",
    "                self.hidden_layer_1.neurons[h].weights[w_ih] -= self.hidden_layer_1.neurons[h].gradient[w_ih]\n",
    "\n",
    "    def undo(self):\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            self.output_layer.neurons[o].bias += self.output_layer.neurons[o].bias_gradient\n",
    "            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
    "                self.output_layer.neurons[o].weights[w_ho] += self.output_layer.neurons[o].gradient[w_ho]\n",
    "        \n",
    "        for h in range(len(self.hidden_layer_2.neurons)):\n",
    "            self.hidden_layer_2.neurons[h].bias += self.hidden_layer_2.neurons[h].bias_gradient\n",
    "            for w_hh in range(len(self.hidden_layer_2.neurons[h].weights)):\n",
    "                self.hidden_layer_2.neurons[h].weights[w_hh] += self.hidden_layer_2.neurons[h].gradient[w_hh]\n",
    "            \n",
    "        for h in range(len(self.hidden_layer_1.neurons)):\n",
    "            self.hidden_layer_1.neurons[h].bias += self.hidden_layer_1.neurons[h].bias_gradient\n",
    "            for w_ih in range(len(self.hidden_layer_1.neurons[h].weights)):\n",
    "                self.hidden_layer_1.neurons[h].weights[w_ih] += self.hidden_layer_1.neurons[h].gradient[w_ih]\n",
    "\n",
    "    def calculate_total_error(self, training_sets):\n",
    "        total_error = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                error = self.output_layer.neurons[o].calculate_error(training_outputs[o])\n",
    "                # if self.loss_function == \"mse\" or self.loss_function == \"msa\":\n",
    "                error /= len(training_sets)\n",
    "                total_error += error\n",
    "        return total_error\n",
    "    \n",
    "    def print_expected_predicted(self, training_sets):\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                print(training_outputs[o], \" \",self.output_layer.neurons[o].output)\n",
    "    \n",
    "    def count_correct(self, training_sets):\n",
    "        count = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                if (training_outputs[o] == 1 and self.output_layer.neurons[o].output >= 0.5) or (training_outputs[o] == 0 and self.output_layer.neurons[o].output < 0.5):\n",
    "                    count+=1\n",
    "        return count\n",
    "    \n",
    "    def true_positive(self, training_sets):\n",
    "        count = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                if training_outputs[o] == 1 and self.output_layer.neurons[o].output >= 0.5:\n",
    "                    count+=1\n",
    "        return count\n",
    "        \n",
    "    def true_negative(self, training_sets):\n",
    "        count = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                if training_outputs[o] == 0 and self.output_layer.neurons[o].output < 0.5:\n",
    "                    count+=1\n",
    "        return count\n",
    "    \n",
    "    def false_positive(self, training_sets):\n",
    "        count = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                if training_outputs[o] == 0 and self.output_layer.neurons[o].output >= 0.5:\n",
    "                    count+=1\n",
    "        return count\n",
    "        \n",
    "    def false_negative(self, training_sets):\n",
    "        count = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                if training_outputs[o] == 1 and self.output_layer.neurons[o].output < 0.5:\n",
    "                    count+=1\n",
    "        return count\n",
    "\n",
    "class NeuronLayer:\n",
    "    def __init__(self, num_neurons, loss_function, activation, bias):\n",
    "        self.loss_function = loss_function\n",
    "        self.activation = activation\n",
    "        self.bias = bias if bias else random.random()\n",
    "        self.neurons = []\n",
    "        for i in range(num_neurons):\n",
    "            self.neurons.append(Neuron(self.loss_function, self.activation, self.bias))\n",
    "\n",
    "    def inspect(self):\n",
    "        print('Neurons:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(' Neuron', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "            print('  Bias:', self.bias)\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, loss_function, activation, bias):\n",
    "        self.loss_function = loss_function\n",
    "        self.activation = activation\n",
    "        self.bias = bias\n",
    "        self.bias_gradient = 0\n",
    "        self.weights = []\n",
    "        self.gradient = []\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = self.squash(self.calculate_total_net_input())\n",
    "        return self.output\n",
    "\n",
    "    def calculate_total_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "\n",
    "    def squash(self, total_net_input):\n",
    "        if self.activation == \"relu\":\n",
    "            return max(0, total_net_input)\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + math.exp(-total_net_input))\n",
    "        if self.activation == \"leaky\":\n",
    "            a = 0.1\n",
    "            return max(a*total_net_input, total_net_input)\n",
    "        if self.activation == \"elu\":\n",
    "            a = 0.1\n",
    "            if total_net_input <= 0:\n",
    "                return a*(math.exp(total_net_input)-1)\n",
    "            else:\n",
    "                return total_net_input\n",
    "        if self.activation == \"hyperbolic\":\n",
    "            return math.tanh(total_net_input) # return (math.exp(total_net_input)-math.exp(-total_net_input))/(math.exp(total_net_input)+math.exp(-total_net_input))\n",
    "\n",
    "    def calculate_pd_error_wrt_total_net_input(self, target_output):\n",
    "        return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input();\n",
    "\n",
    "    def calculate_error(self, target_output):\n",
    "        if self.loss_function == \"mse\":\n",
    "            return (target_output - self.output) ** 2\n",
    "        if self.loss_function == \"cross\":\n",
    "            if target_output == 1:\n",
    "                return -math.log(self.output)\n",
    "            else:\n",
    "                return -math.log(1 - self.output)\n",
    "        if self.loss_function == \"mae\":\n",
    "            return abs(self.output - target_output)\n",
    "        if self.loss_function == \"kl\":\n",
    "            return self.output * math.log(self.output / max(target_output,1e-9))\n",
    "        if self.loss_function == \"huber\":\n",
    "            d = 0.1\n",
    "            if abs(target_output - self.output) < d:\n",
    "                return 0.5 * (target_output - self.output) ** 2\n",
    "            else:\n",
    "                return d * (target_output - self.output - 0.5 * d)\n",
    "\n",
    "    def calculate_pd_error_wrt_output(self, target_output):\n",
    "        if self.loss_function == \"mse\":\n",
    "            return - 2 * target_output + 2 * self.output\n",
    "        if self.loss_function == \"cross\":\n",
    "            if target_output == 1:\n",
    "                return -1/self.output\n",
    "            else:\n",
    "                return -1/(self.output-1)\n",
    "        if self.loss_function == \"mae\":\n",
    "            if self.output > target_output:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        if self.loss_function == \"kl\":\n",
    "            return math.log(self.output / max(target_output,1e-9)) + 1\n",
    "        if self.loss_function == \"huber\":\n",
    "            d = 0.1\n",
    "            if abs(target_output - self.output) < d:\n",
    "                return - target_output + self.output\n",
    "            else:\n",
    "                return - d\n",
    "\n",
    "    def calculate_pd_total_net_input_wrt_input(self):\n",
    "        if self.activation == \"relu\":\n",
    "            if self.output >= 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self.output * (1 - self.output)\n",
    "        if self.activation == \"leaky\":\n",
    "            a = 0.1\n",
    "            if self.output >= 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return a\n",
    "        if self.activation == \"elu\":\n",
    "            a = 0.1\n",
    "            if self.output <= 0:\n",
    "                return self.output+a\n",
    "            else:\n",
    "                return 1\n",
    "        if self.activation == \"hyperbolic\":\n",
    "            return 1-self.output*self.output\n",
    "\n",
    "    def calculate_pd_total_net_input_wrt_weight(self, index):\n",
    "        return self.inputs[index]\n",
    "\n",
    "df = pd.read_excel('HW3train.xlsx')\n",
    "x0 = minmax_scale(df['X_0'].tolist())\n",
    "x1 = minmax_scale(df['X_1'].tolist())\n",
    "y = df['y'].tolist()\n",
    "training_sets = []\n",
    "for i in range(len(x0)):\n",
    "    training_sets.append([[x0[i],x1[i]],[y[i]]])\n",
    "\n",
    "df = pd.read_excel('HW3validate.xlsx')\n",
    "x0 = minmax_scale(df['X_0'].tolist())\n",
    "x1 = minmax_scale(df['X_1'].tolist())\n",
    "y = df['y'].tolist()\n",
    "validation_sets = []\n",
    "for i in range(len(x0)):\n",
    "    validation_sets.append([[x0[i],x1[i]],[y[i]]])\n",
    "\n",
    "y_train = []\n",
    "y_eval = []\n",
    "x_epochs = []\n",
    "\n",
    "nn = NeuralNetwork(2, 10, 10, 1, learning_rate=0.25, loss_function='cross', hidden_layer_1_activation='leaky', hidden_layer_2_activation='leaky', output_layer_activation='sigmoid')\n",
    "prev_error = 2\n",
    "error = 1\n",
    "iteration = 0\n",
    "epochs = 0\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "if BATCH_SIZE > 0:\n",
    "    while epochs < 25:\n",
    "        prev_error = error\n",
    "        nn.train(training_sets[BATCH_SIZE*iterations:BATCH_SIZE+BATCH_SIZE*iterations])\n",
    "        y_train.append(nn.calculate_total_error(training_sets))\n",
    "        y_eval.append(nn.calculate_total_error(validation_sets))\n",
    "        x_epochs.append(epochs)\n",
    "        iterations += 1\n",
    "        if iterations % (math.ceil(len(training_sets) / BATCH_SIZE)) == 0:\n",
    "            iterations = 0\n",
    "            epochs += 1\n",
    "else:\n",
    "    while error < prev_error:\n",
    "        prev_error = error\n",
    "        nn.train(training_sets)\n",
    "        error = nn.calculate_total_error(training_sets)\n",
    "        y_train.append(error)\n",
    "        y_eval.append(nn.calculate_total_error(validation_sets))\n",
    "        x_epochs.append(iteration)\n",
    "        iteration += 1\n",
    "        epochs += 1\n",
    "#         if iteration % 500 == 0:\n",
    "#             print(iteration, error)\n",
    "#             plots(\"Cross-Entropy\")\n",
    "        if iteration == 1000:\n",
    "            break\n",
    "nn.undo()\n",
    "\n",
    "# print(\"Validation sets: \", nn.calculate_total_error(validation_sets))\n",
    "# nn.print_expected_predicted(validation_sets)\n",
    "print(\"True positives:\",nn.true_positive(validation_sets))\n",
    "print(\"True negatives:\",nn.true_negative(validation_sets))\n",
    "print(\"False positives:\",nn.false_positive(validation_sets))\n",
    "print(\"False negatives:\",nn.false_negative(validation_sets))\n",
    "print(\"Count correct:\",nn.count_correct(validation_sets))\n",
    "print(\"Num epochs:\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(loss_fun: str):\n",
    "    plt.plot(x_epochs, y_train, label='Training')\n",
    "    plt.plot(x_epochs, y_eval, label='Validation')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(loss_fun)\n",
    "\n",
    "    plt.title(\"Plot of '{}' over training and validation data\".format(loss_fun))\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
